{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The mathematics behind the Least Squares Method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular section we'll use the least squares method to estimate the coefficients. Here's a quick breakdown of how this method works mathematically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at the plot we created above using seaborn. Now consider each point, and know that they each have a coordinate in the form (X,Y). Now draw an imaginary line between each point and our current \"best-fit\" line. We'll call the distanace between each point and our current best-fit line, D. To get a quick image of what we're currently trying to visualize, take a look at the image below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quick display of image form wikipedia\n",
    "from IPython.display import Image\n",
    "url = 'http://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/220px-Linear_least_squares_example2.svg.png'\n",
    "Image(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as before, we're labeling each green line as having a distance D, and each red point as having a coordinate of (X,Y). Then we can define our best fit line as the line having the property were:\n",
    "$$ D_{1}^2 + D_{2}^2 + D_{3}^2 + D_{4}^2 + ....+ D_{N}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we find this line? The least-square line approximating the set of points\n",
    "\n",
    "$$ (X,Y)_{1},(X,Y)_{2},(X,Y)_{3},(X,Y)_{4},(X,Y)_{5}, $$\n",
    "\n",
    "has the equation:\n",
    "$$ Y = a_{0} +a_{1}X $$\n",
    "this is basically just a rewritten form of the standard equation for a line:\n",
    "$$Y=mx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve for these constants a0 and a1 by simultaneously solving these equations:\n",
    "$$ \\Sigma Y = a_{0}N + a_{1}\\Sigma X $$\n",
    "$$ \\Sigma XY = a_{0}\\Sigma X + a_{1}\\Sigma X^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are called the normal equations for the least squares line. There are further steps that can be taken in rearranging these equations  to solve for y, but we'll let scikit-learn do the rest of the heavy lifting here. If you want further information on the mathematics of the above formulas, check out this [video](https://www.youtube.com/watch?v=Qa2APhWjQPc).\n",
    "\n",
    "For now, we'll use numpy to do a simple single variable linear regression. Afterwards we'll unleash the power of scikit learn to do a full multivariate linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Using Numpy for a Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has a built in Least Square Method in its linear algebra library. We'll use this first for our Univariate regression and then move on to scikit learn for Multivariate regression.\n",
    "\n",
    "We will start by setting up the X and Y arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mandatory imports copied from the previous project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "# load data as pandas DataFrame\n",
    "boston_df = DataFrame(boston.data)\n",
    "\n",
    "# label columns\n",
    "boston_df.columns = boston.feature_names\n",
    "boston_df['Price'] = boston.target\n",
    "\n",
    "# Set up X as median room values\n",
    "X = boston_df.RM\n",
    "\n",
    "# Set up Y as the target price of the houses.\n",
    "Y = boston_df.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have X and Y, let's go ahead and use numpy to create the single variable linear regression.\n",
    "\n",
    "We know that a line has the equation:\n",
    "$$y=mx+b$$\n",
    "which we can rewrite using matrices:\n",
    "$$y=Ap$$\n",
    "where:\n",
    "$$A = \\begin{bmatrix}x & 1\\end{bmatrix}$$\n",
    "and\n",
    "$$p= \\begin{bmatrix}m \\\\b\\end{bmatrix}$$\n",
    "\n",
    "This is the same as the first equation if you carry out the linear algebra. \n",
    "So we'll start by creating the A matrix using numpy. We'll do this by creating a matrix in the form [X 1], so we'll call every value in our original X using a list comprehension and then set up an array in the form [X 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the X array in the form [X 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can get the best fit values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now find out m and b values for our best fit line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's plot it! Note that we use the original format of the boston information. We only did matrix transformations to utilize the numpy least square method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the original points (Price vs Number of Rooms)\n",
    "\n",
    "\n",
    "# Next best fit line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Error evaluation ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Great! We've just completed a single variable regression using the least squares method with Python! Let's see if we can find the error in our fitted line. Checking out the documentation [here](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.lstsq.html), we see that the resulting array has the total squared error. For each element, it checks the the difference between the line and the true value (our original D value), squares it, and returns the sum of all these. This was the summed D^2 value we discussed earlier. \n",
    "\n",
    "It's probably easier to understand the root mean square error, which is similar to the standard deviation. In this case, to find the root mean square error we divide by the number of elements and then take the square root.\n",
    "\n",
    "For now let's see how we can get the root mean square error of the line we just fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the resulting array\n",
    "result = np.linalg.lstsq(X,Y)\n",
    "\n",
    "# Get the total error\n",
    "error_total = result[1]\n",
    "\n",
    "# Get the root mean square error\n",
    "rmse = np.sqrt(error_total/len(X) )\n",
    "\n",
    "# Print\n",
    "print(\"The root mean square error was %.2f \" %rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the root mean square error (RMSE) corresponds approximately to the standard deviation we can now say that the price of a house won't vary more than 2 times the RMSE. Note: If you need more information check out this [link](http://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule).\n",
    "\n",
    "Thus we can reasonably expect a house price to be within $13,200 of our line fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Using scikit learn to implement a multivariate regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll keep moving along with using scikit learn to do a multi variable regression. This will be a similar apporach to the above example, but scikit learn will be able to take into account more than a single variable!\n",
    "\n",
    "We'll start by importing the [linear regression library](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn.\n",
    "\n",
    "The sklearn.linear_model.LinearRegression class is an estimator. Estimators predict a value based on the observed data. In scikit-learn, all estimators implement the fit() and predict() methods. The former method is used to learn the parameters of a model, and the latter method is used to predict the value using the learned parameters. It is easy to experiment with different models using scikit-learn because all estimators implement the fit and predict methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import for Linear Regression\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a LinearRegression object, afterwards, type linear_regression and press tab to see the list of methods availble on this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a LinearRegression Object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions we will be using are:\n",
    "\n",
    "linear_regression.fit() which fits a linear model\n",
    "\n",
    "linear_regression.predict() which is used to predict Y using the linear model with estimated coefficients\n",
    "\n",
    "linear_regression.score() which returns the coefficient of determination (R^2). A measure of how well observed outcomes are replicated by the model, learn more about it [here](http://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll start the multi variable regression analysis by seperating our boston dataframe into the features and the target columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data Columns\n",
    "X = boston_df.drop('Price',1)\n",
    "\n",
    "# Targets\n",
    "Y = boston_df.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we're ready to pass the X and Y using the linear regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's go ahead check the intercept and number of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(' The number of coefficients used : %d ' % len(linear_regression.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So we have basically made an equation for a line, but instead of just one coefficient m and an intercept b, we now have 13 coefficients. To get an idea of what this looks like check out the [documentation](http://scikit-learn.org/stable/modules/linear_model.html) for this equation:\n",
    "$$ y(w,x) = w_0 + w_1 x_1 + ... + w_p x_p $$\n",
    "\n",
    "Where $$w = (w_1, ...w_p)$$ are the coefficients and $$ w_0 $$ is the intercept "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll do next is set up a DataFrame showing all the Features and their estimated coefficients obtained form the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set a DataFrame from the Features\n",
    "coeff_df = DataFrame(boston_df.columns)\n",
    "coeff_df.columns = ['Features']\n",
    "\n",
    "# Set a new column lining up the coefficients from the linear regression\n",
    "\n",
    "\n",
    "# Print\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the highest correlation between a feature and a house price is the number of rooms.\n",
    "\n",
    "Now let's move on to Predicting prices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Using Training and Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a dataset a training set is implemented to build up a model, while a validation set is used to validate the model built. Data points in the training set are excluded from the validation set.\n",
    "\n",
    "Fortunately, scikit learn has a built in function specifically for this called train_test_split().\n",
    "\n",
    "The parameters passed are X and Y, then optionally test_size parameter, representing the proportion of the dataset to include in the test set. You can learn more about this [here](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print shapes of the training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Predicting Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training and test sets, let's go ahead and try to use them to predict house prices. We'll use our training set for fitting the model and then use test set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# Fit the model on training set\n",
    "linear_regression.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run a prediction on both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions\n",
    "Y_pred_train = linear_regression.predict(X_train)\n",
    "Y_pred_test = linear_regression.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "print(\"Mean squared error on Training set : %.2f\"  % np.mean((Y_train - Y_pred_train) ** 2)) \n",
    "print(\"Mean squared error on Test set : %.2f\"  %np.mean((Y_test - Y_pred_test) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our mean squared error between our training and testing is very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test set score\n",
    "linear_regression.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 : Residual Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression analysis, the difference between the observed value and the predicted value is called the residual.\n",
    "\n",
    "$$Residual = Observed\\:value - Predicted\\:value $$\n",
    "\n",
    "Each data point has one residual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate.\n",
    "\n",
    "Residual plots are a good way to visualize the errors in your data.  If you have done a good job then your data should be randomly scattered around line zero. If there is some structure or pattern, that means your model is not capturing something.\n",
    "So now let's go ahead and create the residual plot. For more info on the residual plots check out this great [link](http://blog.minitab.com/blog/adventures-in-statistics/why-you-need-to-check-your-residual-plots-for-regression-analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot for training data\n",
    "train = plt.scatter(Y_pred_train,(Y_train-Y_pred_train),c='b',alpha=0.5)\n",
    "\n",
    "# Scatter plot for test data\n",
    "test = plt.scatter(Y_pred_test,(Y_test-Y_pred_test),c='r',alpha=0.5)\n",
    "\n",
    "# Plot a horizontal axis\n",
    "plt.hlines(y=0,xmin=-10,xmax=50)\n",
    "\n",
    "#Labels\n",
    "plt.legend((train,test),('Training','Test'),loc='lower left')\n",
    "plt.title('Residual Plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There are no patterns in the residual plot. Residuals seem to be randomly allocated above and below the horizontal. We could also use seaborn to create these plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Residual plot of the entire dataset using seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for linear regression. Please refer to scikit learn documentation for more details :  http://scikit-learn.org/stable/modules/linear_model.html#linear-model"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
